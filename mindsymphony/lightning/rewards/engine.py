"""
Reward Engine - å¥–åŠ±ä¿¡å·å·¥ç¨‹

æ ¸å¿ƒèŒè´£:
1. æ˜¾å¼å¥–åŠ±æ”¶é›† - ç”¨æˆ·ç›´æ¥åé¦ˆ
2. éšå¼ä¿¡å·æå– - ä»äº¤äº’æ¨¡å¼æ¨æ–­
3. è®¡ç®—å¥–åŠ±ç”Ÿæˆ - è·¨ä»»åŠ¡èšåˆ
4. å¥–åŠ±å½’ä¸€åŒ–å’ŒéªŒè¯

å— Agent Lightning å¯å‘ï¼Œè‡ªåŠ¨å°†äº¤äº’è½¬åŒ–ä¸ºå¥–åŠ±ä¿¡å·
"""

import re
import json
import time
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass
from enum import Enum
from datetime import datetime


class RewardType(Enum):
    """å¥–åŠ±ç±»å‹"""
    EXPLICIT = "explicit"      # ç”¨æˆ·ç›´æ¥åé¦ˆ
    IMPLICIT = "implicit"      # ä»äº¤äº’æ¨¡å¼æ¨æ–­
    COMPUTED = "computed"      # è·¨ä»»åŠ¡è®¡ç®—


@dataclass
class RewardSignal:
    """å¥–åŠ±ä¿¡å·æ•°æ®ç»“æ„"""
    reward_type: RewardType
    value: float  # -1.0 to 1.0
    confidence: float  # 0.0 to 1.0
    source: str  # æ¥æºè¯´æ˜
    context: Dict[str, Any]  # ä¸Šä¸‹æ–‡
    timestamp: float


class RewardEngine:
    """å¥–åŠ±ä¿¡å·å¼•æ“

    ä»å¤šç»´åº¦æå–å¥–åŠ±ä¿¡å·ï¼Œç”¨äºé©±åŠ¨è¿›åŒ–å­¦ä¹ 

    ç»´åº¦:
    1. æ˜¾å¼åé¦ˆ: ç”¨æˆ·è¯„åˆ†ã€ğŸ‘/ğŸ‘ã€æ–‡å­—åé¦ˆ
    2. ä»»åŠ¡å®Œæˆ: å®Œæˆåº¦ã€æˆåŠŸ/å¤±è´¥
    3. å‚ä¸åº¦: äº¤äº’æ·±åº¦ã€è¿½é—®ã€å»¶ç»­
    4. æ•ˆç‡: Tokenæ•ˆç‡ã€æ—¶é—´æ•ˆç‡
    5. ååŒ: å¤šæŠ€èƒ½åä½œæµç•…åº¦

    ç¤ºä¾‹:
        engine = RewardEngine()

        # è®°å½•æ˜¾å¼åé¦ˆ
        engine.record_explicit_feedback("thumbs_up", confidence=1.0)

        # ä»ç”¨æˆ·æ¶ˆæ¯æå–éšå¼ä¿¡å·
        signals = engine.extract_implicit_signals(user_message, interaction_context)

        # è®¡ç®—ç»¼åˆå¥–åŠ±
        total_reward = engine.compute_total_reward(episode_data)
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}

        # æƒé‡é…ç½®
        self.weights = {
            RewardType.EXPLICIT: self.config.get('explicit_weight', 1.0),
            RewardType.IMPLICIT: self.config.get('implicit_weight', 0.6),
            RewardType.COMPUTED: self.config.get('computed_weight', 0.4)
        }

        # éšå¼ä¿¡å·æå–å™¨
        self._implicit_extractors: List[Callable] = [
            self._extract_task_completion_signals,
            self._extract_engagement_signals,
            self._extract_sentiment_signals,
            self._extract_efficiency_signals
        ]

        # å†å²è®°å½•ï¼ˆç”¨äºè®¡ç®—å¥–åŠ±ï¼‰
        self._interaction_history: List[Dict] = []
        self._max_history = 1000

    def record_explicit_feedback(
        self,
        feedback_type: str,  # 'thumbs_up', 'thumbs_down', 'rating', 'text'
        value: Optional[float] = None,
        raw_feedback: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> RewardSignal:
        """è®°å½•æ˜¾å¼ç”¨æˆ·åé¦ˆ

        Args:
            feedback_type: åé¦ˆç±»å‹
            value: æ•°å€¼ï¼ˆå¦‚è¯„åˆ† 1-5ï¼‰
            raw_feedback: åŸå§‹åé¦ˆæ–‡æœ¬
            metadata: é¢å¤–ä¸Šä¸‹æ–‡
        """
        context = metadata or {}
        context['feedback_type'] = feedback_type
        context['raw_feedback'] = raw_feedback

        # è½¬æ¢ä¸º -1.0 ~ 1.0 èŒƒå›´
        normalized_value, confidence = self._normalize_explicit_feedback(
            feedback_type, value, raw_feedback
        )

        reward = RewardSignal(
            reward_type=RewardType.EXPLICIT,
            value=normalized_value,
            confidence=confidence,
            source=f"explicit:{feedback_type}",
            context=context,
            timestamp=time.time()
        )

        return reward

    def _normalize_explicit_feedback(
        self,
        feedback_type: str,
        value: Optional[float],
        raw_feedback: Optional[str]
    ) -> tuple[float, float]:
        """å°†æ˜¾å¼åé¦ˆå½’ä¸€åŒ–åˆ° [-1, 1]"""

        if feedback_type == 'thumbs_up':
            return 1.0, 1.0

        elif feedback_type == 'thumbs_down':
            return -1.0, 1.0

        elif feedback_type == 'rating':
            # å‡è®¾ 1-5 è¯„åˆ†
            if value is None:
                return 0.0, 0.0
            # 1->-1, 3->0, 5->1
            normalized = (value - 3) / 2
            return max(-1.0, min(1.0, normalized)), 1.0

        elif feedback_type == 'text':
            # ä»æ–‡æœ¬æå–æƒ…æ„Ÿ
            return self._analyze_text_sentiment(raw_feedback or "")

        return 0.0, 0.0

    def _analyze_text_sentiment(self, text: str) -> tuple[float, float]:
        """ç®€å•çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æ"""
        text = text.lower()

        # ç§¯æè¯æ±‡
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å®Œç¾', 'æ„Ÿè°¢', 'good', 'great', 'excellent', 'perfect', 'thanks', 'love', 'awesome']
        # æ¶ˆæè¯æ±‡
        negative_words = ['å·®', 'ç³Ÿ', 'é”™è¯¯', 'é—®é¢˜', 'bad', 'poor', 'wrong', 'error', 'terrible', 'awful', 'hate', 'sucks']

        pos_count = sum(1 for w in positive_words if w in text)
        neg_count = sum(1 for w in negative_words if w in text)

        if pos_count == 0 and neg_count == 0:
            return 0.0, 0.3  # ä¸­æ€§ï¼Œä½ç½®ä¿¡åº¦

        # è®¡ç®—æƒ…æ„Ÿå€¼
        total = pos_count + neg_count
        sentiment = (pos_count - neg_count) / max(total, 1)

        # ç½®ä¿¡åº¦åŸºäºåŒ¹é…æ•°é‡
        confidence = min(0.9, 0.3 + total * 0.2)

        return sentiment, confidence

    def extract_implicit_signals(
        self,
        user_message: str,
        context: Dict[str, Any]
    ) -> List[RewardSignal]:
        """ä»ç”¨æˆ·äº¤äº’ä¸­æå–éšå¼å¥–åŠ±ä¿¡å·

        è¿™ç±»ä¼¼äº Agent Lightning çš„éšå¼ä¿¡å·æå–
        """
        signals = []

        for extractor in self._implicit_extractors:
            try:
                signal = extractor(user_message, context)
                if signal:
                    signals.append(signal)
            except Exception as e:
                print(f"[RewardEngine] Extractor error: {e}")

        # ä¿å­˜åˆ°å†å²
        self._interaction_history.append({
            'message': user_message,
            'context': context,
            'signals': [s.source for s in signals],
            'timestamp': time.time()
        })

        # é™åˆ¶å†å²å¤§å°
        if len(self._interaction_history) > self._max_history:
            self._interaction_history = self._interaction_history[-self._max_history:]

        return signals

    def _extract_task_completion_signals(
        self,
        message: str,
        context: Dict
    ) -> Optional[RewardSignal]:
        """æå–ä»»åŠ¡å®Œæˆä¿¡å·"""
        message = message.lower()

        # å®Œæˆä¿¡å·
        completion_patterns = [
            r'å®Œæˆ', r'å¥½äº†', r'æå®š', r'ok', r'okay', r'done', r'finished',
            r'å®Œç¾', r'æ­£æ˜¯', r'exactly', r'perfect', r'thank', r'æ„Ÿè°¢'
        ]

        # ä¸­æ–­/æ”¾å¼ƒä¿¡å·
        interruption_patterns = [
            r'ç®—äº†', r'åœæ­¢', r'æ”¾å¼ƒ', r'stop', r'quit', r'give up',
            r'ä¸å¯¹', r'é”™äº†', r'wrong', r'incorrect'
        ]

        for pattern in completion_patterns:
            if re.search(pattern, message):
                return RewardSignal(
                    reward_type=RewardType.IMPLICIT,
                    value=0.5,
                    confidence=0.7,
                    source="implicit:task_completion",
                    context={"matched_pattern": pattern, "message_preview": message[:50]},
                    timestamp=time.time()
                )

        for pattern in interruption_patterns:
            if re.search(pattern, message):
                return RewardSignal(
                    reward_type=RewardType.IMPLICIT,
                    value=-0.5,
                    confidence=0.7,
                    source="implicit:interruption",
                    context={"matched_pattern": pattern, "message_preview": message[:50]},
                    timestamp=time.time()
                )

        return None

    def _extract_engagement_signals(
        self,
        message: str,
        context: Dict
    ) -> Optional[RewardSignal]:
        """æå–å‚ä¸åº¦ä¿¡å·"""
        # è·å–ä¼šè¯å†å²
        session_id = context.get('session_id')
        if not session_id:
            return None

        # ç»Ÿè®¡å½“å‰ä¼šè¯çš„äº¤äº’è½®æ•°
        session_messages = [
            h for h in self._interaction_history
            if h['context'].get('session_id') == session_id
        ]

        turn_count = len(session_messages)

        # å¤šè½®äº¤äº’ = é«˜å‚ä¸åº¦
        if turn_count >= 5:
            return RewardSignal(
                reward_type=RewardType.IMPLICIT,
                value=0.3,
                confidence=0.6,
                source="implicit:high_engagement",
                context={"turn_count": turn_count},
                timestamp=time.time()
            )

        # è¿½é—®/æ·±å…¥ä¿¡å·
        followup_patterns = [
            r'ä¸ºä»€ä¹ˆ', r'æ€ä¹ˆ', r'èƒ½å¦', r'è¿˜æœ‰', r'è¯¦ç»†',
            r'why', r'how', r'can you', r'what about', r'more', r'details'
        ]

        for pattern in followup_patterns:
            if re.search(pattern, message.lower()):
                return RewardSignal(
                    reward_type=RewardType.IMPLICIT,
                    value=0.2,
                    confidence=0.5,
                    source="implicit:followup",
                    context={"matched_pattern": pattern},
                    timestamp=time.time()
                )

        return None

    def _extract_sentiment_signals(
        self,
        message: str,
        context: Dict
    ) -> Optional[RewardSignal]:
        """æå–æƒ…æ„Ÿä¿¡å·"""
        sentiment, confidence = self._analyze_text_sentiment(message)

        # åªæå–æ˜ç¡®çš„æƒ…æ„Ÿ
        if abs(sentiment) > 0.3 and confidence > 0.5:
            return RewardSignal(
                reward_type=RewardType.IMPLICIT,
                value=sentiment * 0.5,  # éšå¼æƒ…æ„Ÿæƒé‡è¾ƒä½
                confidence=confidence * 0.8,
                source="implicit:sentiment",
                context={"raw_sentiment": sentiment},
                timestamp=time.time()
            )

        return None

    def _extract_efficiency_signals(
        self,
        message: str,
        context: Dict
    ) -> Optional[RewardSignal]:
        """æå–æ•ˆç‡ä¿¡å·"""
        # ä»ä¸Šä¸‹æ–‡è·å–æ€§èƒ½æŒ‡æ ‡
        token_count = context.get('token_count', 0)
        latency_ms = context.get('latency_ms', 0)
        expected_latency = context.get('expected_latency_ms', 5000)

        if latency_ms == 0 or expected_latency == 0:
            return None

        # æ•ˆç‡ = 1 - (å®é™…å»¶è¿Ÿ / é¢„æœŸå»¶è¿Ÿ)
        efficiency = 1.0 - (latency_ms / expected_latency)

        # åªæœ‰æ˜¾è‘—çš„æ•ˆç‡å·®å¼‚æ‰äº§ç”Ÿå¥–åŠ±
        if efficiency > 0.3:  # æ¯”é¢„æœŸå¿«30%
            return RewardSignal(
                reward_type=RewardType.IMPLICIT,
                value=0.2,
                confidence=0.5,
                source="implicit:high_efficiency",
                context={"efficiency": efficiency, "latency_ms": latency_ms},
                timestamp=time.time()
            )
        elif efficiency < -0.5:  # æ¯”é¢„æœŸæ…¢50%
            return RewardSignal(
                reward_type=RewardType.IMPLICIT,
                value=-0.2,
                confidence=0.5,
                source="implicit:low_efficiency",
                context={"efficiency": efficiency, "latency_ms": latency_ms},
                timestamp=time.time()
            )

        return None

    def compute_total_reward(
        self,
        episode_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆå¥–åŠ±

        èšåˆæ˜¾å¼ã€éšå¼ã€è®¡ç®—ä¸‰ç±»å¥–åŠ±
        """
        signals = episode_data.get('signals', [])

        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = {
            RewardType.EXPLICIT: [],
            RewardType.IMPLICIT: [],
            RewardType.COMPUTED: []
        }

        for signal in signals:
            if isinstance(signal, dict):
                signal = RewardSignal(**signal)
            by_type[signal.reward_type].append(signal)

        # è®¡ç®—å„ç±»å‹åŠ æƒå¥–åŠ±
        weighted_rewards = {}
        total_confidence = 0

        for rtype, rlist in by_type.items():
            if not rlist:
                weighted_rewards[rtype] = 0.0
                continue

            # æŒ‰ç½®ä¿¡åº¦åŠ æƒå¹³å‡
            total_weight = sum(s.confidence for s in rlist)
            if total_weight == 0:
                weighted_rewards[rtype] = 0.0
                continue

            avg_reward = sum(s.value * s.confidence for s in rlist) / total_weight
            weighted_rewards[rtype] = avg_reward * self.weights[rtype]
            total_confidence += total_weight

        # æ€»å¥–åŠ±
        total_reward = sum(weighted_rewards.values())

        # å½’ä¸€åŒ–åˆ° [-1, 1]
        total_weight_sum = sum(self.weights.values())
        if total_weight_sum > 0:
            total_reward = total_reward / total_weight_sum

        return {
            "total_reward": max(-1.0, min(1.0, total_reward)),
            "by_type": {
                k.value: v for k, v in weighted_rewards.items()
            },
            "signal_counts": {
                k.value: len(v) for k, v in by_type.items()
            },
            "confidence": min(1.0, total_confidence / max(len(signals), 1)),
            "timestamp": time.time()
        }

    def compute_synergy_reward(
        self,
        skill_sequence: List[str],
        handoff_scores: List[float]
    ) -> RewardSignal:
        """è®¡ç®—å¤šæŠ€èƒ½ååŒå¥–åŠ±"""
        if not handoff_scores:
            return RewardSignal(
                reward_type=RewardType.COMPUTED,
                value=0.0,
                confidence=0.0,
                source="computed:synergy",
                context={"reason": "no_handoffs"},
                timestamp=time.time()
            )

        # ååŒè´¨é‡ = äº¤æ¥æµç•…åº¦
        avg_handoff = sum(handoff_scores) / len(handoff_scores)

        # å¤šæ ·æ€§å¥–åŠ±ï¼ˆä½¿ç”¨å¤šä¸ªä¸åŒæŠ€èƒ½ï¼‰
        unique_skills = len(set(skill_sequence))
        diversity_bonus = min(0.2, (unique_skills - 1) * 0.05)

        value = avg_handoff * 0.8 + diversity_bonus

        return RewardSignal(
            reward_type=RewardType.COMPUTED,
            value=value,
            confidence=0.6,
            source="computed:synergy",
            context={
                "skill_count": len(skill_sequence),
                "unique_skills": unique_skills,
                "avg_handoff": avg_handoff
            },
            timestamp=time.time()
        )

    def compute_novelty_reward(
        self,
        current_pattern: Dict,
        historical_patterns: List[Dict]
    ) -> RewardSignal:
        """è®¡ç®—æ–°é¢–æ€§å¥–åŠ± - é¼“åŠ±æ¢ç´¢æ–°æ¨¡å¼"""
        if not historical_patterns:
            # ç¬¬ä¸€ä¸ªæ¨¡å¼ï¼Œç»™äºˆä¸­ç­‰å¥–åŠ±
            return RewardSignal(
                reward_type=RewardType.COMPUTED,
                value=0.3,
                confidence=0.5,
                source="computed:novelty",
                context={"reason": "first_pattern"},
                timestamp=time.time()
            )

        # è®¡ç®—ä¸å†å²æ¨¡å¼çš„ç›¸ä¼¼åº¦
        similarities = []
        for pattern in historical_patterns[-100:]:  # æœ€è¿‘100ä¸ª
            sim = self._pattern_similarity(current_pattern, pattern)
            similarities.append(sim)

        max_similarity = max(similarities) if similarities else 0

        # æ–°é¢–åº¦ = 1 - æœ€å¤§ç›¸ä¼¼åº¦
        novelty = 1.0 - max_similarity

        # åªæœ‰æ˜¾è‘—æ–°é¢–çš„æ¨¡å¼æ‰å¥–åŠ±
        if novelty > 0.7:
            return RewardSignal(
                reward_type=RewardType.COMPUTED,
                value=0.3,
                confidence=novelty * 0.8,
                source="computed:novelty",
                context={"novelty_score": novelty, "max_similarity": max_similarity},
                timestamp=time.time()
            )

        return RewardSignal(
            reward_type=RewardType.COMPUTED,
            value=0.0,
            confidence=0.3,
            source="computed:novelty",
            context={"novelty_score": novelty, "reason": "not_novel"},
            timestamp=time.time()
        )

    def _pattern_similarity(self, p1: Dict, p2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ¨¡å¼çš„ç›¸ä¼¼åº¦"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        s1 = set(p1.get('skills', []))
        s2 = set(p2.get('skills', []))

        if not s1 and not s2:
            return 1.0
        if not s1 or not s2:
            return 0.0

        intersection = len(s1 & s2)
        union = len(s1 | s2)

        return intersection / union if union > 0 else 0.0

    def get_reward_summary(self, hours: int = 24) -> Dict:
        """è·å–å¥–åŠ±ä¿¡å·æ‘˜è¦"""
        since = time.time() - hours * 3600

        recent = [
            h for h in self._interaction_history
            if h['timestamp'] > since
        ]

        return {
            "period_hours": hours,
            "total_interactions": len(recent),
            "signals_extracted": sum(len(h.get('signals', [])) for h in recent),
            "avg_signals_per_interaction": (
                sum(len(h.get('signals', [])) for h in recent) / len(recent)
                if recent else 0
            )
        }


# å…¨å±€ RewardEngine å®ä¾‹
_default_reward_engine: Optional[RewardEngine] = None


def get_reward_engine(config: Optional[Dict] = None) -> RewardEngine:
    """è·å–å…¨å±€ RewardEngine å®ä¾‹"""
    global _default_reward_engine
    if _default_reward_engine is None:
        _default_reward_engine = RewardEngine(config)
    return _default_reward_engine
